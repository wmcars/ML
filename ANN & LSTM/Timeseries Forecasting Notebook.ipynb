{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://storage.googleapis.com/kf-pipeline-contrib-public/ai-hub-assets/oob-timeseries-prediction/timeseries.png)\n",
    "# Intended Use\n",
    "- Performs 1-step time series forecasting.\n",
    "- Performs quantile regression on provided quantiles.\n",
    "- Supports anomaly detection.\n",
    "\n",
    "# Runtime Parameters\n",
    "### Data Parameters\n",
    "| Parameter               | Definition|\n",
    "| :---                    |:---|\n",
    "| training-dir            | GCS directory containing training data|\n",
    "| validation-dir          | (optional) GCS directory containing validation data. If not provided validation data is split from training data.|\n",
    "| testing-dir             | (optional) GCS directory container testing data. If provided final evaluation of model performed with this data.|\n",
    "| output-dir              | GCS directory to write servable SavedModel.|\n",
    "\n",
    "### Model Hyperparameters\n",
    "| Parameter               | Definition|\n",
    "| :---                    |:---|\n",
    "| input-length          | Number of input window steps the model can use.|\n",
    "| lower-quantile          | The lower quantile to regress on. |\n",
    "| upper-quantile          | The upper quantile to regress on. |\n",
    "| batch-size              | Batch size during training.|\n",
    "| epochs                  | This component uses early stopping to terminate training. This parameter can be used to specify the max number of epochs. Training will step at this value or when early stopping is triggered.\n",
    "\n",
    "# Input\n",
    "Each timeseries should be stored as it's own json file in the data directories. For example:\n",
    "\n",
    "```\n",
    "├── gcs\n",
    "    ├── training-dir\n",
    "    |   ├── train-timeseries-1.json\n",
    "    |   ├── train-timeseries-2.json\n",
    "    |   └── ...\n",
    "    ├── testing-dir\n",
    "    |   ├── test-timeseries-1.json\n",
    "    |   ├── test-timeseries-2.json\n",
    "    |   └── ...\n",
    "    └── ...\n",
    "```\n",
    "\n",
    "The timeseries data in each json should be in the following format:\n",
    "```\n",
    "{\n",
    "    \"data\":[\n",
    "        [1325376000000,3.6484771574],\n",
    "        [1325383200000,4.6002538071],\n",
    "        ...\n",
    "        [1420070400000,0.3172588832],\n",
    "    ],\n",
    "    \"columns\":[\"timestamp\", \"target\"]\n",
    "}\n",
    "```\n",
    "* **data**: This is the time series data where each step contains an array with a timestamp in milliseconds and the value of the timeseries at that timestamp. Time intervals between values should be constant. \n",
    "* **columns**: This contains the name of the respective columns which should align with the ordering in *data*.\n",
    "\n",
    "# Output\n",
    "A servable SavedModel will be stored in *output-dir*. It's input signature will take a *input-length* length sequence of values and return the prediction of the next step. An example payload:\n",
    "```\n",
    "{\n",
    "    \"instances\":[\n",
    "        [3.6484771574, 4.6002538071, ..., 0.3172588832],\n",
    "        [19.8257467994, 22.8485064011, ..., 22.4039829303],\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "# Anomaly Detection\n",
    "This model supports anomaly detection with quantile regression. An anomalous value is defined as any point on the time series whose value is outside of the predicted quantile range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Component Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS path to directory containing training data, validation data, and test data. Reference README for format.\n",
    "training_dir = 'gs://kf-pipeline-contrib-public-data/electricity/train'\n",
    "validation_dir = 'gs://kf-pipeline-contrib-public-data/electricity/validation'\n",
    "test_dir = 'gs://kf-pipeline-contrib-public-data/electricity/test'\n",
    "\n",
    "# GCS path to store trainined model. You must have write access.CHANGE BEFORE RUNNING\n",
    "model_dir = 'GCS DIRECTORY WITH WRITE ACCESS ' # gs://...\n",
    "\n",
    "input_length = 84  # how many steps the model can consider\n",
    "\n",
    "# set you lower and upper quantiles which will be used for anomaly detection\n",
    "lower_quantile = 0.01\n",
    "upper_quantile = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install KFP python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "KFP_PACKAGE = 'https://storage.googleapis.com/ml-pipeline/release/0.1.12/kfp.tar.gz'\n",
    "!pip3 install $KFP_PACKAGE --upgrade\n",
    "!pip3 install pandas\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install https://storage.googleapis.com/ml-pipeline/release/0.1.10/kfp.tar.gz --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.notebook\n",
    "import kfp.gcp as gcp\n",
    "import kfp.components as comp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name='timeseries forecasting experiment')\n",
    "\n",
    "timeseries_training_op = comp.load_component_from_url(\n",
    "    'https://storage.googleapis.com/kf-pipeline-contrib-public/ai-hub-assets/oob-timeseries-prediction/component.yaml')\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='pipeline name', description='pipeline description')\n",
    "def one_step_training(\n",
    "    training_dir='',\n",
    "    validation_dir='',\n",
    "    testing_dir='',\n",
    "    input_length=84,\n",
    "    lower_quantile=0.01,\n",
    "    upper_quantile=0.99,\n",
    "    batch_size=128,\n",
    "    output_dir='',\n",
    "    epochs=10000\n",
    "    ):\n",
    "    training_op = timeseries_training_op(\n",
    "        training_dir=training_dir,\n",
    "        validation_dir=validation_dir,\n",
    "        testing_dir=testing_dir,\n",
    "        input_length=input_length,\n",
    "        lower_quantile=lower_quantile,\n",
    "        upper_quantile=upper_quantile,\n",
    "        batch_size=batch_size,\n",
    "        output_dir=output_dir,\n",
    "        epochs=epochs).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "compiler.Compiler().compile(one_step_training, 'one_step_pipeline.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run = client.run_pipeline(exp.id, 'ts run 1', 'one_step_pipeline.tar.gz', params=\n",
    "                        {\n",
    "                            'training-dir':training_dir,\n",
    "                            'validation-dir':validation_dir,\n",
    "                            'testing-dir':test_dir,\n",
    "                            'input-length':input_length,\n",
    "                            'lower-quantile': lower_quantile,\n",
    "                            'upper-quantile': upper_quantile,\n",
    "                            'output-dir':model_dir\n",
    "                        } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model after run and test with toy data\n",
    "When training is done load the trained model and just try some toy data to make sure everything worked.\n",
    "\n",
    "First lets create a class to easily use our model and then pass in the toy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesModel(object):\n",
    "    \n",
    "    def __init__(self, model_dir):\n",
    "        self.model_dir = model_dir\n",
    "    \n",
    "    def predict(self, ts):\n",
    "        with tf.Session(graph=tf.Graph()) as sess:\n",
    "            tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], self.model_dir)\n",
    "            input_tensor = sess.graph.get_tensor_by_name('raw_input:0')\n",
    "            lower_quantile_tensor = sess.graph.get_tensor_by_name('lower_quantile:0')\n",
    "            median_tensor = sess.graph.get_tensor_by_name('median:0')\n",
    "            upper_quantile_tensor = sess.graph.get_tensor_by_name('upper_quantile:0')\n",
    "            lower_quantile, median, upper_quantile = sess.run(\n",
    "                [lower_quantile_tensor, median_tensor, upper_quantile_tensor],\n",
    "                feed_dict={input_tensor:ts})\n",
    "        \n",
    "        return {\n",
    "            'lower_quantile':lower_quantile,\n",
    "            'median': median,\n",
    "            'upper_quantile': upper_quantile\n",
    "        }\n",
    "    \n",
    "ts_model = TimeseriesModel(model_dir)\n",
    "import pprint\n",
    "# predict on two ranges\n",
    "pprint.pprint(ts_model.predict(np.stack([np.arange(84), np.arange(20, 104)]).reshape(2, -1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreive public dataset for visualizing\n",
    "Retreive a public dataset of electricity usage from UCI's Machine Learning Repository. It will be used to train the model so we can visualize how well the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip\n",
    "!unzip LD2011_2014.txt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in public dataset to notebook\n",
    "Read in the public dataset and then split it into the contexts used to train the time series model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def read_electricity_data():\n",
    "    \"\"\"\n",
    "    Read electricy data from local file. Use this for local notebook development.\n",
    "    \"\"\"\n",
    "    file_path = 'LD2011_2014.txt'\n",
    "    data = pd.read_csv(file_path, sep=';', index_col=0, parse_dates=True, decimal=',')\n",
    "    \n",
    "    data_2H = data.resample('2H').sum()/8\n",
    "    return [np.trim_zeros(data_2H.iloc[:,i], trim='f') for i in range(data_2H.shape[1])]\n",
    "\n",
    "freq = '2H'\n",
    "input_length = 7 * 12\n",
    "test_future_time = 12 * 7 * 4 #every two hours for 1 month\n",
    "start_dataset = pd.Timestamp(\"2014-01-01 00:00:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2014-09-01 00:00:00\", freq=freq)\n",
    "\n",
    "elec = read_electricity_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset\n",
    "For the model trained above the dataset was trained on 9 months, validated on 1 month, and tested on 1 month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were the split used to train the model above\n",
    "training_elec = [ts[start_dataset:end_training - 1] for ts in elec]\n",
    "validation_elec = [ts[end_training - input_length:end_training + test_future_time] for ts in elec]\n",
    "test_elec = [ts[end_training - input_length + test_future_time:end_training + 2 * test_future_time] for ts in elec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize what this looks like for a single timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(pd.DataFrame(training_elec[13]), label='train')\n",
    "plt.plot(pd.DataFrame(validation_elec[13]), alpha=0.5, label='validation')\n",
    "plt.plot(pd.DataFrame(test_elec[13]), alpha=0.4, label='test')\n",
    "plt.title('Temporal Split')\n",
    "plt.gcf().autofmt_xdate()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create helper classes to predict and plot\n",
    "These classes will be used to wrap the prediction and plot the test timeseries with predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesGraph(object):\n",
    "  \n",
    "    def __init__(self, ts, input_length, title, model):\n",
    "        self.ts = ts\n",
    "        self.input_length = input_length\n",
    "        self.create_dataset()\n",
    "        self.x = list(range(self.input_length, len(self.ts)))\n",
    "        self.title = title\n",
    "        self.model=model\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        context_windows = [self.ts[i:i+self.input_length] for i in range(len(self.ts) - self.input_length)]\n",
    "        labels = [self.ts[i] for i in range(self.input_length, len(self.ts))]\n",
    "        self.labels = np.array(labels).reshape(-1, 1)\n",
    "        self.ts_array = np.array(context_windows)\n",
    "\n",
    "    def plot_truth_and_predictions(self, anomalies=False):\n",
    "        predictions = self.model.predict(self.ts_array)\n",
    "        \n",
    "        if anomalies:\n",
    "            is_anomaly = np.logical_or(\n",
    "            predictions['lower_quantile'] > self.labels, predictions['upper_quantile'] < self.labels)\n",
    "            indices = np.argwhere(is_anomaly)[:,0]\n",
    "            plt.plot(indices + self.input_length, self.labels[indices], 'ro', label='anomalies')\n",
    "        self.plot(predictions)\n",
    "\n",
    "    def plot(self, predictions):\n",
    "        plt.fill_between(self.x,\n",
    "                         predictions['lower_quantile'].reshape(-1),\n",
    "                         predictions['upper_quantile'].reshape(-1),\n",
    "                         color='lightskyblue',\n",
    "                         label='quantile range',\n",
    "                         alpha=0.4)\n",
    "        plt.plot(self.x, predictions['median'], color='orange', label='prediction',\n",
    "            alpha=0.8)\n",
    "        plt.plot(self.ts, color='black', label='target', alpha=0.3)\n",
    "        plt.legend(loc=2)\n",
    "        plt.title(self.title)\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(18.5, 10.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the test predictions with the actual time series\n",
    "We will now perform a 1-step prediction through the timeseries and plot the test time series and compare it to the results of our predictions.\n",
    "\n",
    "What's in the graph:\n",
    "- the black line is the true times series\n",
    "- the blue line is the forecast of the model\n",
    "- the blue area is the predicted quantile range between *lower_quantile* and *upper_quantile*\n",
    "- the part of our chart with no predictions or percentile is the first context window slice used for prediction\n",
    "\n",
    "Try different time series indices as there are over 300 timeseries in this multi times eries datasets. You can also try different slices in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ts_plotter = TimeseriesGraph(\n",
    "    test_elec[0].values.reshape(-1, 1),\n",
    "    input_length,\n",
    "    'Test Timeseries Index 0',\n",
    "    TimeseriesModel(model_dir))\n",
    "\n",
    "ts_plotter.plot_truth_and_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection\n",
    "This model is capable of performing anomaly detection. An anomaly is defined as any value on our time series that is outside for our predicted quantile range. \n",
    "\n",
    "Let's plot the same graph and include the anomalies as red dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_plotter.plot_truth_and_predictions(anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper method for data conversion\n",
    "If you want to convert your own data and train a new model you can use this helper method below to convert your pandas time series data into the json format specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage \n",
    "    \n",
    "def write_pandas_multi_timeseries_to_gcs(\n",
    "    multi_timeseries, # instance of multi timseries - a list of pd.Dataframe timeseries\n",
    "    bucket='ai_hub_dev', # bucket to write data\n",
    "    gcs_folder_path='timeseries/data/electricity_train', # path inside bucket to write data\n",
    "    local_folder_path= 'data'): # local directory to store intermediate files\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    for i, ts in enumerate(multi_timeseries):\n",
    "        if not isinstance(ts, pd.DataFrame):\n",
    "            ts = pd.DataFrame(ts)\n",
    "        ts.reset_index(inplace=True)\n",
    "        ts.columns = ['timestamp', 'target']\n",
    "        local_file_path = '{}/{}.json'.format(local_folder_path, i)\n",
    "        ts.to_json(local_file_path, orient='split', index=False)\n",
    "        blob = bucket.blob('{}/{}.json'.format(gcs_folder_path, i))\n",
    "        blob.upload_from_filename('data/{}.json'.format(i))\n",
    "\n",
    "# write_pandas_multi_timeseries_to_gcs(training_elec)  # example call"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
