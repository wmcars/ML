{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from get_data import GetData\n",
    "from preprocessing import PreProcessing\n",
    "from autoencoder import AutoEncoder\n",
    "from data_processing import DataProcessing\n",
    "from model import NeuralNetwork\n",
    "from model_20_encoded import nnmodel\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import keras.layers as kl\n",
    "import keras as kr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model,load_model\n",
    "from keras import regularizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fix_yahoo_finance as fix\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader.data as pdr\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix.pdr_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 downloaded\n"
     ]
    }
   ],
   "source": [
    "class GetData:\n",
    "    def __init__(self, ticker, start, end):\n",
    "        self.ticker = ticker\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    # get stock data\n",
    "    def get_stock_data(self):\n",
    "        stock_data = pdr.get_data_yahoo(self.ticker, self.start, self.end)\n",
    "        stock_data.to_csv(\"stock_data.csv\")\n",
    "\n",
    "    # get twitter data\n",
    "    # do your code here!\n",
    "\n",
    "    # get news data\n",
    "    # do your code here!\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = GetData(\"AAPL\", \"2000-01-01\", \"2018-10-01\")\n",
    "    data.get_stock_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "    def __init__(self, split, feature_split, csv_, out_, out_test,log_train):\n",
    "        self.split = split\n",
    "        self.feature_split = feature_split\n",
    "        self.stock_data = pd.read_csv(csv_)\n",
    "        self.csv = csv_\n",
    "        self.out = out_\n",
    "        self.out_test = out_test\n",
    "        self.log_train = log_train\n",
    "\n",
    "    # wavelet transform and create autoencoder data\n",
    "    def make_wavelet_train(self):\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        log_train_data = []\n",
    "        for i in range(22,(len(self.stock_data)//10)*10 - 11):\n",
    "            train = []\n",
    "            log_ret = []\n",
    "            for j in range(1, 6):\n",
    "                # if i > 11:\n",
    "                x = np.array(self.stock_data.iloc[i-11:i,j])\n",
    "                # IPython.embed()\n",
    "                (ca, cd) = pywt.dwt(x, \"haar\")\n",
    "                cat = pywt.threshold(ca, np.std(ca), mode=\"soft\")\n",
    "                cdt = pywt.threshold(cd, np.std(cd), mode=\"soft\")\n",
    "                tx = pywt.idwt(cat, cdt, \"haar\")\n",
    "                log = np.diff(np.log(tx))*100\n",
    "                macd = np.mean(x[5:]) - np.mean(x)\n",
    "                # ma = np.mean(x)\n",
    "                sd = np.std(x)\n",
    "                log_ret = np.append(log_ret, log)\n",
    "                x_tech = np.append(macd*10, sd)\n",
    "                train = np.append(train, x_tech)\n",
    "            train_data.append(train)\n",
    "            log_train_data.append(log_ret)\n",
    "        trained = pd.DataFrame(train_data)\n",
    "        trained.to_csv(\"preprocessing/indicators.csv\")\n",
    "        log_train = pd.DataFrame(log_train_data, index=None)\n",
    "        log_train.to_csv(self.log_train)\n",
    "        # auto_train = pd.DataFrame(train_data[0:800])\n",
    "        # auto_test = pd.DataFrame(train_data[801:1000])\n",
    "        # auto_train.to_csv(\"auto_train.csv\")\n",
    "        # auto_test.to_csv(\"auto_test.csv\")\n",
    "        rbm_train = pd.DataFrame(log_train_data[0:int(self.split*self.feature_split*len(log_train_data))], index=None)\n",
    "        rbm_train.to_csv(self.out)\n",
    "        rbm_test = pd.DataFrame(log_train_data[int(self.split*self.feature_split*len(log_train_data))+1:\n",
    "                                               int(self.feature_split*len(log_train_data))])\n",
    "        rbm_test.to_csv(self.out_test)\n",
    "        for i in range((len(self.stock_data) // 10) * 10 - 11):\n",
    "            y = 100*np.log(self.stock_data.iloc[i + 11, 5] / self.stock_data.iloc[i + 10, 5])\n",
    "            test_data.append(y)\n",
    "        # test = pd.DataFrame(test_data)\n",
    "        # test.to_csv(\"preprocessing/test_data.csv\")\n",
    "\n",
    "    def make_test_data(self):\n",
    "        test_stock = []\n",
    "        # stock_data_test = pd.read_csv(\"stock_data_test.csv\", index_col=0)\n",
    "\n",
    "        for i in range((len(self.stock_data) // 10) * 10 - 11):\n",
    "            l = self.stock_data.iloc[i-11, 5]\n",
    "            test_stock.append(l)\n",
    "            test = pd.DataFrame(test_stock)\n",
    "            test.to_csv(\"preprocessing/test_stock.csv\")\n",
    "\n",
    "        stock_test_data = np.array(test_stock)[int(self.feature_split*len(test_stock) +\n",
    "                                               self.split*(1-self.feature_split)*len(test_stock)):]\n",
    "        stock = pd.DataFrame(stock_test_data, index=None)\n",
    "        stock.to_csv(\"stock_data_test.csv\")\n",
    "\n",
    "        # print(train_data[1:5])\n",
    "        # print(test_data[1:5])\n",
    "        # plt.plot(train_data[1])\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mac/anaconda3/envs/AI-Test/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/envs/AI-Test/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 2s 7ms/step\n",
      "3.497406137859361\n",
      "(4677, 20)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_10 to have shape (1,) but got array with shape (55,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4d5902f1c1dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"features/autoencoded_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"60_return_forex/encoded_return_test_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"preprocessing/log_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"forex_y/log_test_y.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"forex_y/test_price.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"60_return_forex/predicted_price.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"60_return_forex/price.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"60_return_forex/ret_acc.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Price Accuracy Average = {average} \\nPrice Accuracy Standard Deviation = {std}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4d5902f1c1dc>\u001b[0m in \u001b[0;36mnnmodel\u001b[0;34m(epochs, regularizer1, regularizer2, encoded_train, encoded_test, train_y, log_test, test_price, predicted, price, return_acc)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/final_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mpredicted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI-Test/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI-Test/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AI-Test/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_10 to have shape (1,) but got array with shape (55,)"
     ]
    }
   ],
   "source": [
    "class AutoEncoder:\n",
    "    def __init__(self, encoding_dim,testing,in_,in_test,out_,log_train):\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.inp = in_\n",
    "        self.inp_test = in_test\n",
    "        self.out = out_\n",
    "        self.testing = testing\n",
    "        self.log_train = log_train\n",
    "\n",
    "    def build_train_model(self, input_shape, encoded1_shape, encoded2_shape, decoded1_shape, decoded2_shape):\n",
    "        input_data = Input(shape=(1, input_shape))\n",
    "\n",
    "        encoded1 = Dense(encoded1_shape, activation=\"relu\", activity_regularizer=regularizers.l2(0))(input_data)\n",
    "        encoded2 = Dense(encoded2_shape, activation=\"relu\", activity_regularizer=regularizers.l2(0))(encoded1)\n",
    "        encoded3 = Dense(self.encoding_dim, activation=\"relu\", activity_regularizer=regularizers.l2(0))(encoded2)\n",
    "        decoded1 = Dense(decoded1_shape, activation=\"relu\", activity_regularizer=regularizers.l2(0))(encoded3)\n",
    "        decoded2 = Dense(decoded2_shape, activation=\"relu\", activity_regularizer=regularizers.l2(0))(decoded1)\n",
    "        decoded = Dense(input_shape, activation=\"sigmoid\", activity_regularizer=regularizers.l2(0))(decoded2)\n",
    "\n",
    "        autoencoder = Model(inputs=input_data, outputs=decoded)\n",
    "\n",
    "        \n",
    "        encoder = Model(input_data, encoded3)\n",
    "\n",
    "        # Now train the model using data we already preprocessed\n",
    "        autoencoder.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "        train = pd.read_csv(self.inp, index_col=0)\n",
    "        ntrain = np.array(train)\n",
    "        train_data = np.reshape(ntrain, (len(ntrain), 1, input_shape))\n",
    "\n",
    "        # print(train_data)\n",
    "        # autoencoder.summary()\n",
    "        if self.testing:\n",
    "            encoder = load_model(\"models/encoder.h5\")\n",
    "            encoder.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "        else:\n",
    "            autoencoder.fit(train_data, train_data, epochs=1000)\n",
    "            encoder.save(\"models/encoder.h5\")\n",
    "\n",
    "        test = pd.read_csv(self.inp_test, index_col=0)\n",
    "        ntest = np.array(test)\n",
    "        test_data = np.reshape(ntest, (len(ntest), 1, 55))\n",
    "\n",
    "        print(autoencoder.evaluate(test_data, test_data))\n",
    "        # pred = np.reshape(ntest[1], (1, 1, 75))\n",
    "        # print(encoder.predict(pred))\n",
    "\n",
    "        log_train = pd.read_csv(self.log_train, index_col=0)\n",
    "        coded_train = []\n",
    "        for i in range(len(log_train)):\n",
    "            data = np.array(log_train.iloc[i, :])\n",
    "            values = np.reshape(data, (1, 1, 55))\n",
    "            coded = encoder.predict(values)\n",
    "            shaped = np.reshape(coded, (20,))\n",
    "            coded_train.append(shaped)\n",
    "\n",
    "        train_coded = pd.DataFrame(coded_train)\n",
    "        train_coded.to_csv(self.out)\n",
    "\n",
    "\n",
    "def nnmodel(epochs, regularizer1, regularizer2,encoded_train,encoded_test,train_y,log_test,test_price,predicted,price,return_acc):\n",
    "\n",
    "    train_data = np.array(pd.read_csv(encoded_train, index_col=0))\n",
    "    # length = len(train_data)\n",
    "    train_data = np.reshape(train_data, (len(train_data), 20))\n",
    "    print(np.shape(train_data))\n",
    "    test_data = np.array(pd.read_csv(encoded_test, index_col=0))\n",
    "    test_data = np.reshape(test_data, (len(test_data), 20))\n",
    "    train_y = np.array(pd.read_csv(train_y, index_col=0))\n",
    "    test_y = np.array(pd.read_csv(log_test, index_col=0))\n",
    "    price = np.array(pd.read_csv(test_price, index_col=0))\n",
    "\n",
    "    model = kr.models.Sequential()\n",
    "    # model.add(kl.Dense(50, activation=\"sigmoid\", activity_regularizer=kr.regularizers.l2(0)))\n",
    "    model.add(kl.Dense(20, input_dim=20, activation=\"tanh\", activity_regularizer=kr.regularizers.l2(regularizer1)))\n",
    "    model.add(kl.Dense(20, activation=\"tanh\", activity_regularizer=kr.regularizers.l2(regularizer1)))\n",
    "    model.add(kl.Dense(20, activation=\"tanh\", activity_regularizer=kr.regularizers.l2(regularizer2)))\n",
    "    # model.add(kl.Dense(100))\n",
    "    model.add(kl.Dense(1))\n",
    "\n",
    "    model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")\n",
    "    model.fit(train_data, train_y, epochs=epochs)\n",
    "    model.save(\"models/final_model.h5\")\n",
    "    predicted_data = []\n",
    "    predicted_price = []\n",
    "    for i in range(len(test_data)):\n",
    "        prediction = model.predict(np.reshape(test_data[i], (1, 20)))\n",
    "        predicted_data.append(prediction)\n",
    "        price_pred = np.exp(prediction)*price[i]\n",
    "        predicted_price.append(price_pred)\n",
    "        # print(test_data[i])\n",
    "\n",
    "    # print(model.evaluate(test_data, test_y))\n",
    "    pd.DataFrame(np.reshape(predicted_price, (len(predicted_price, )))).to_csv(predicted)\n",
    "    pd.DataFrame(price).to_csv(price)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(np.arange(len(predicted_data)), np.reshape(test_y, (len(test_y))),\n",
    "             np.reshape(predicted_data, (len(predicted_data))))\n",
    "    plt.title(\"Prediction vs Actual\")\n",
    "    plt.ylabel(\"Log Return\")\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(np.arange(len(predicted_price)), np.reshape(price, (len(price))),\n",
    "             np.reshape(predicted_price, (len(predicted_price))))\n",
    "    plt.xlabel(\"Time stamp\")\n",
    "    plt.ylabel(\"Market Price\")\n",
    "    plt.show()\n",
    "\n",
    "    price_r_score = r2_score(np.reshape(predicted_price, (len(predicted_price))), price)\n",
    "    return_r_score = r2_score(np.reshape(predicted_data, (len(predicted_data))), test_y)\n",
    "    price_mse = mean_squared_error(np.reshape(predicted_price, (len(predicted_price))), price)\n",
    "    return_mse = mean_squared_error(np.reshape(predicted_data, (len(predicted_data))), test_y)\n",
    "\n",
    "    print(f\"Regularizer for 1: {regularizer1} \\nRegularizer for 2: {regularizer2} \\nEpochs: {epochs}\")\n",
    "    print(f\"Predicted Price r^2 value: {price_r_score} \\nPredicted return r^2 value: {return_r_score}\"\n",
    "          f\"\\nPredict Price MSE: {price_mse} \\nPredicted Return MSE: {return_mse}\")\n",
    "    dataset = []\n",
    "    values = np.array([regularizer1, regularizer2, epochs, price_r_score, return_r_score, price_mse, return_mse])\n",
    "    dataset.append(values)\n",
    "    dataset = pd.DataFrame(dataset, columns=[\"regularizer1\", \"regularizer2\", \"epochs\", \"price_r_score\", \"return_r_score\", \"price_mse\", \"return_mse\"])\n",
    "    # print(dataset)\n",
    "    accuracy = []\n",
    "    for i in range(len(price)-1):\n",
    "        acc = 100 - (np.abs(predicted_price[i] - price[i+1]))/price[i+1] * 100\n",
    "        accuracy.append(acc)\n",
    "    average = np.mean(accuracy)\n",
    "    std = np.std(accuracy)\n",
    "    ret_acc = []\n",
    "    for i in range(len(test_y)-1):\n",
    "        if test_y[i] != 0:\n",
    "            acc = 100 - (np.abs(predicted_data[i] - test_y[i]))/test_y[i] * 100\n",
    "            ret_acc.append(acc)\n",
    "    ret_avg = np.mean(ret_acc)\n",
    "    ret_std = np.std(ret_acc)\n",
    "    pd.DataFrame(np.reshape(ret_acc, (len(ret_acc, )))).to_csv(return_acc)\n",
    "    prediction = np.exp(model.predict(np.reshape(test_data[-2], (1, 20))))*price[-2]\n",
    "    print(prediction)\n",
    "\n",
    "    return dataset, average, std\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "preprocess = PreProcessing(0.8, 0.25,\"stock_data.csv\",\"preprocessing/rbm_train.csv\",\"preprocessing/rbm_test.csv\",\"preprocessing/log_train.csv\")\n",
    "preprocess.make_wavelet_train()\n",
    "preprocess.make_test_data()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "autoencoder = AutoEncoder(20,True,\"preprocessing/rbm_train.csv\",\"preprocessing/rbm_test.csv\",\"features/autoencoded_data.csv\",\"preprocessing/log_train.csv\")\n",
    "autoencoder.build_train_model(55, 40, 30, 30, 40)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "dataset, average, std = nnmodel(500, 0.05, 0.01,\"features/autoencoded_data.csv\",\"60_return_forex/encoded_return_test_data.csv\",\"preprocessing/log_train.csv\",\"forex_y/log_test_y.csv\",\"forex_y/test_price.csv\",\"60_return_forex/predicted_price.csv\",\"60_return_forex/price.csv\",\"60_return_forex/ret_acc.csv\")\n",
    "print(f\"Price Accuracy Average = {average} \\nPrice Accuracy Standard Deviation = {std}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
